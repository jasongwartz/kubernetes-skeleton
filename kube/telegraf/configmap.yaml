kind: ConfigMap
apiVersion: v1
metadata:
  name: telegraf
data:
  telegraf.conf: |

    # Global tags can be specified here in key="value" format.
    [global_tags]
    kube-host = "$HOSTNAME"

    # Configuration for telegraf agent
    [agent]
      ## Default data collection interval for all inputs
      interval = "10s"
      ## Rounds collection interval to 'interval'
      ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
      round_interval = true

      ## Telegraf will send metrics to outputs in batches of at most
      ## metric_batch_size metrics.
      ## This controls the size of writes that Telegraf sends to output plugins.
      metric_batch_size = 1000

      ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
      ## output, and will flush this buffer on a successful write. Oldest metrics
      ## are dropped first when this buffer fills.
      ## This buffer only fills when writes fail to output plugin(s).
      metric_buffer_limit = 10000

      ## Collection jitter is used to jitter the collection by a random amount.
      ## Each plugin will sleep for a random time within jitter before collecting.
      ## This can be used to avoid many plugins querying things like sysfs at the
      ## same time, which can have a measurable effect on the system.
      collection_jitter = "0s"

      ## Default flushing interval for all outputs. You shouldn't set this below
      ## interval. Maximum flush_interval will be flush_interval + flush_jitter
      flush_interval = "10s"
      ## Jitter the flush interval by a random amount. This is primarily to avoid
      ## large write spikes for users running a large number of telegraf instances.
      ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
      flush_jitter = "0s"

      ## By default, precision will be set to the same timestamp order as the
      ## collection interval, with the maximum being 1s.
      ## Precision will NOT be used for service inputs, such as logparser and statsd.
      ## Valid values are "ns", "us" (or "Âµs"), "ms", "s".
      precision = ""

      ## Logging configuration:
      ## Run telegraf with debug log messages.
      debug = false
      ## Run telegraf in quiet mode (error log messages only).
      quiet = false
      ## Specify the log file name. The empty string means to log to stderr.
      logfile = ""

      ## Override default hostname, if empty use os.Hostname()
      hostname = ""
      ## If set to true, do no set the "host" tag in the telegraf agent.
      omit_hostname = false


    ###############################################################################
    #                            OUTPUT PLUGINS                                   #
    ###############################################################################

    # # Configuration for the Prometheus client to spawn
    [[outputs.prometheus_client]]
    #   ## Address to listen on
      listen = ":9126"

    ###############################################################################
    #                            INPUT PLUGINS                                    #
    ###############################################################################

    # Read metrics about cpu usage
    [[inputs.cpu]]
      ## Whether to report per-cpu stats or not
      percpu = true
      ## Whether to report total system cpu stats or not
      totalcpu = true
      ## If true, collect raw CPU time metrics.
      collect_cpu_time = false


    # Read metrics about disk usage by mount point
    [[inputs.disk]]
      ## By default, telegraf gather stats for all mountpoints.
      ## Setting mountpoints will restrict the stats to the specified mountpoints.
      # mount_points = ["/"]

      ## Ignore some mountpoints by filesystem type. For example (dev)tmpfs (usually
      ## present on /run, /var/run, /dev/shm or /dev).
      ignore_fs = ["tmpfs", "devtmpfs"]


    # Read metrics about disk IO by device
    [[inputs.diskio]]
      ## By default, telegraf will gather stats for all devices including
      ## disk partitions.
      ## Setting devices will restrict the stats to the specified devices.
      # devices = ["sda", "sdb"]
      ## Uncomment the following line if you need disk serial numbers.
      # skip_serial_number = false


    # Get kernel statistics from /proc/stat
    [[inputs.kernel]]
      # no configuration


    # Read metrics about memory usage
    [[inputs.mem]]
      # no configuration


    # Get the number of processes and group them by status
    [[inputs.processes]]
      # no configuration


    # Read metrics about swap memory usage
    [[inputs.swap]]
      # no configuration


    # Read metrics about system load & uptime
    [[inputs.system]]
      # no configuration



    # # Read specific statistics per cgroup
    [[inputs.cgroup]]
    #   ## Directories in which to look for files, globs are supported.
    #   ## Consider restricting paths to the set of cgroups you really
    #   ## want to monitor if you have a large number of cgroups, to avoid
    #   ## any cardinality issues.
    #   # paths = [
    #   #   "/cgroup/memory",
    #   #   "/cgroup/memory/child1",
    #   #   "/cgroup/memory/child2/*",
    #   # ]
    #   ## cgroup stat fields, as file names, globs are supported.
    #   ## these file names are appended to each path from above.
    #   # files = ["memory.*usage*", "memory.limit_in_bytes"]



    # # Read metrics about docker containers
    [[inputs.docker]]
    #   ## Docker Endpoint
    #   ##   To use TCP, set endpoint = "tcp://[ip]:[port]"
    #   ##   To use environment variables (ie, docker-machine), set endpoint = "ENV"
        endpoint = "unix:///var/run/docker.sock"
    #   ## Only collect metrics for these containers, collect all if empty
    #   container_names = []
    #   ## Timeout for docker list, info, and stats commands
        timeout = "5s"
    #
    #   ## Whether to report for each container per-device blkio (8:0, 8:1...) and
    #   ## network (eth0, eth1, ...) stats or not
    #   perdevice = true
    #   ## Whether to report for each container total blkio and network stats or not
    #   total = false
    #



    # # Read stats from one or more Elasticsearch servers or clusters
    # [[inputs.elasticsearch]]
    #   ## specify a list of one or more Elasticsearch servers
    #   servers = ["http://localhost:9200"]
    #
    #   ## Timeout for HTTP requests to the elastic search server(s)
    #   http_timeout = "5s"
    #
    #   ## set local to false when you want to read the indices stats from all nodes
    #   ## within the cluster
    #   local = true
    #
    #   ## set cluster_health to true when you want to also obtain cluster level stats
    #   cluster_health = false
    #
    #   ## Optional SSL Config
    #   # ssl_ca = "/etc/telegraf/ca.pem"
    #   # ssl_cert = "/etc/telegraf/cert.pem"
    #   # ssl_key = "/etc/telegraf/key.pem"
    #   ## Use SSL but skip chain & host verification
    #   # insecure_skip_verify = false


    # # Read metrics from one or more commands that can output to stdout
    #[[inputs.exec]]
    #   ## Commands array
    #  command = "cat /sys/class/thermal/thermal_zone0/temp"
    #  data_format = "value"
    #  data_type = "integer"
    #  name_override = "system_temperature"

    #   ## Timeout for each command to complete.
    #   timeout = "5s"
    #
    #   ## measurement name suffix (for separating different commands)
    #   name_suffix = "_mycollector"
    #
    #   ## Data format to consume.
    #   ## Each data format has it's own unique set of configuration options, read
    #   ## more about them here:
    #   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
    #   data_format = "influx"


    # # HTTP/HTTPS request given an address a method and a timeout
    # [[inputs.http_response]]
    #   ## Server address (default http://localhost)
    #   address = "http://github.com"
    #   ## Set response_timeout (default 5 seconds)
    #   response_timeout = "5s"
    #   ## HTTP Request Method
    #   method = "GET"
    #   ## Whether to follow redirects from the server (defaults to false)
    #   follow_redirects = true
    #   ## HTTP Request Headers (all values must be strings)
    #   # [inputs.http_response.headers]
    #   #   Host = "github.com"
    #   ## Optional HTTP Request Body
    #   # body = '''
    #   # {'fake':'data'}
    #   # '''
    #
    #   ## Optional SSL Config
    #   # ssl_ca = "/etc/telegraf/ca.pem"
    #   # ssl_cert = "/etc/telegraf/cert.pem"
    #   # ssl_key = "/etc/telegraf/key.pem"
    #   ## Use SSL but skip chain & host verification
    #   # insecure_skip_verify = false


    # # Read flattened metrics from one or more JSON HTTP endpoints
    # [[inputs.httpjson]]
    #   ## NOTE This plugin only reads numerical measurements, strings and booleans
    #   ## will be ignored.
    #
    #   ## a name for the service being polled
    #   name = "webserver_stats"
    #
    #   ## URL of each server in the service's cluster
    #   servers = [
    #     "http://localhost:9999/stats/",
    #     "http://localhost:9998/stats/",
    #   ]
    #   ## Set response_timeout (default 5 seconds)
    #   response_timeout = "5s"
    #
    #   ## HTTP method to use: GET or POST (case-sensitive)
    #   method = "GET"
    #
    #   ## List of tag names to extract from top-level of JSON server response
    #   # tag_keys = [
    #   #   "my_tag_1",
    #   #   "my_tag_2"
    #   # ]
    #
    #   ## HTTP parameters (all values must be strings)
    #   [inputs.httpjson.parameters]
    #     event_type = "cpu_spike"
    #     threshold = "0.75"
    #
    #   ## HTTP Header parameters (all values must be strings)
    #   # [inputs.httpjson.headers]
    #   #   X-Auth-Token = "my-xauth-token"
    #   #   apiVersion = "v1"
    #
    #   ## Optional SSL Config
    #   # ssl_ca = "/etc/telegraf/ca.pem"
    #   # ssl_cert = "/etc/telegraf/cert.pem"
    #   # ssl_key = "/etc/telegraf/key.pem"
    #   ## Use SSL but skip chain & host verification
    #   # insecure_skip_verify = false



    # # Read metrics from the kubernetes kubelet api
    [[inputs.kubernetes]]
    #   ## URL for the kubelet
      url = "http://$HOST_IP:10255"
    #
    #   ## Use bearer token for authorization
    #   # bearer_token = /path/to/bearer/token
    #
    #   ## Optional SSL Config
    #   # ssl_ca = /path/to/cafile
    #   # ssl_cert = /path/to/certfile
    #   # ssl_key = /path/to/keyfile
    #   ## Use SSL but skip chain & host verification
    #   # insecure_skip_verify = false



    # # Read metrics from one or many memcached servers
    # [[inputs.memcached]]
    #   ## An array of address to gather stats about. Specify an ip on hostname
    #   ## with optional port. ie localhost, 10.0.0.1:11211, etc.
    #   servers = ["localhost:11211"]
    #   # unix_sockets = ["/var/run/memcached.sock"]


    # # Read metrics about network interface usage
    [[inputs.net]]
    #   ## By default, telegraf gathers stats from any up interface (excluding loopback)
    #   ## Setting interfaces will tell it to gather these explicit interfaces,
    #   ## regardless of status.
    #   ##
        interfaces = ["eth0"]


    # # TCP or UDP 'ping' given url and collect response time in seconds
    # [[inputs.net_response]]
    #   ## Protocol, must be "tcp" or "udp"
    #   protocol = "tcp"
    #   ## Server address (default localhost)
    #   address = "github.com:80"
    #   ## Set timeout
    #   timeout = "1s"
    #
    #   ## Optional string sent to the server
    #   # send = "ssh"
    #   ## Optional expected string in answer
    #   # expect = "ssh"
    #   ## Set read timeout (only used if expecting a response)
    #   read_timeout = "1s"


    # # Read TCP metrics such as established, time wait and sockets counts.
    # [[inputs.netstat]]
    #   # no configuration


    # # Read Nginx's basic status information (ngx_http_stub_status_module)
    # [[inputs.nginx]]
    #   ## An array of Nginx stub_status URI to gather stats.
    #   urls = ["http://127.0.0.1/nginx_status"]



    # # Ping given url(s) and return statistics
    # [[inputs.ping]]
    #   ## NOTE: this plugin forks the ping command. You may need to set capabilities
    #   ## via setcap cap_net_raw+p /bin/ping
    #   #
    #   ## urls to ping
    #   urls = ["www.google.com"] # required
    #   ## number of pings to send per collection (ping -c <COUNT>)
    #   # count = 1
    #   ## interval, in s, at which to ping. 0 == default (ping -i <PING_INTERVAL>)
    #   # ping_interval = 1.0
    #   ## per-ping timeout, in s. 0 == no timeout (ping -W <TIMEOUT>)
    #   # timeout = 1.0
    #   ## interface to send ping from (ping -I <INTERFACE>)
    #   # interface = ""


    # # Read metrics from one or many prometheus clients
    # [[inputs.prometheus]]
    #   ## An array of urls to scrape metrics from.
    #   urls = ["http://localhost:9100/metrics"]
    #
    #   ## Use bearer token for authorization
    #   # bearer_token = /path/to/bearer/token
    #
    #   ## Optional SSL Config
    #   # ssl_ca = /path/to/cafile
    #   # ssl_cert = /path/to/certfile
    #   # ssl_key = /path/to/keyfile
    #   ## Use SSL but skip chain & host verification
    #   # insecure_skip_verify = false



    # # Read metrics from one or many redis servers
    # [[inputs.redis]]
    #   ## specify servers via a url matching:
    #   ##  [protocol://][:password]@address[:port]
    #   ##  e.g.
    #   ##    tcp://localhost:6379
    #   ##    tcp://:password@192.168.99.100
    #   ##    unix:///var/run/redis.sock
    #   ##
    #   ## If no servers are specified, then localhost is used as the host.
    #   ## If no port is specified, 6379 is used
    #   servers = ["tcp://localhost:6379"]

    # # A plugin to collect stats from Varnish HTTP Cache
    # [[inputs.varnish]]
    #   ## The default location of the varnishstat binary can be overridden with:
    #   binary = "/usr/bin/varnishstat"
    #
    #   ## By default, telegraf gather stats for 3 metric points.
    #   ## Setting stats will override the defaults shown below.
    #   ## Glob matching can be used, ie, stats = ["MAIN.*"]
    #   ## stats may also be set to ["*"], which will collect all stats
    #   stats = ["MAIN.cache_hit", "MAIN.cache_miss", "MAIN.uptime"]


    ###############################################################################
    #                            SERVICE INPUT PLUGINS                            #
    ###############################################################################

    # # Influx HTTP write listener
    # [[inputs.http_listener]]
    #   ## Address and port to host HTTP listener on
    #   service_address = ":8186"
    #
    #   ## maximum duration before timing out read of the request
    #   read_timeout = "10s"
    #   ## maximum duration before timing out write of the response
    #   write_timeout = "10s"
    #
    #   ## Maximum allowed http request body size in bytes.
    #   ## 0 means to use the default of 536,870,912 bytes (500 mebibytes)
    #   max_body_size = 0
    #
    #   ## Maximum line size allowed to be sent in bytes.
    #   ## 0 means to use the default of 65536 bytes (64 kibibytes)
    #   max_line_size = 0


    # # Stream and parse log file(s).
    # [[inputs.logparser]]
    #   ## Log files to parse.
    #   ## These accept standard unix glob matching rules, but with the addition of
    #   ## ** as a "super asterisk". ie:
    #   ##   /var/log/**.log     -> recursively find all .log files in /var/log
    #   ##   /var/log/*/*.log    -> find all .log files with a parent dir in /var/log
    #   ##   /var/log/apache.log -> only tail the apache log file
    #   files = ["/var/log/apache/access.log"]
    #   ## Read file from beginning.
    #   from_beginning = false
    #
    #   ## Parse logstash-style "grok" patterns:
    #   ##   Telegraf built-in parsing patterns: https://goo.gl/dkay10
    #   [inputs.logparser.grok]
    #     ## This is a list of patterns to check the given log file(s) for.
    #     ## Note that adding patterns here increases processing time. The most
    #     ## efficient configuration is to have one pattern per logparser.
    #     ## Other common built-in patterns are:
    #     ##   %{COMMON_LOG_FORMAT}   (plain apache & nginx access logs)
    #     ##   %{COMBINED_LOG_FORMAT} (access logs + referrer & agent)
    #     patterns = ["%{COMBINED_LOG_FORMAT}"]
    #     ## Name of the outputted measurement name.
    #     measurement = "apache_access_log"
    #     ## Full path(s) to custom pattern files.
    #     custom_pattern_files = []
    #     ## Custom patterns can also be defined here. Put one pattern per line.
    #     custom_patterns = '''
    #     '''


    # # Stream a log file, like the tail -f command
    # [[inputs.tail]]
    #   ## files to tail.
    #   ## These accept standard unix glob matching rules, but with the addition of
    #   ## ** as a "super asterisk". ie:
    #   ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
    #   ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
    #   ##   "/var/log/apache.log" -> just tail the apache log file
    #   ##
    #   ## See https://github.com/gobwas/glob for more examples
    #   ##
    #   files = ["/var/mymetrics.out"]
    #   ## Read file from beginning.
    #   from_beginning = false
    #
    #   ## Data format to consume.
    #   ## Each data format has it's own unique set of configuration options, read
    #   ## more about them here:
    #   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
    #   data_format = "influx"

